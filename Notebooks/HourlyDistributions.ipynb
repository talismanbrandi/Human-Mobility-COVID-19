{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import collections, functools, operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/place_types.json', 'r') as f:\n",
    "    place_types = json.load(f)\n",
    "\n",
    "with open('translation.json', 'r') as f:\n",
    "    translation = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicePlaceType(df_test, df_base, place_types_list=[]):\n",
    "    dict_df = {}\n",
    "    for placeType in place_types_list:\n",
    "        df = df_test[df_test['top_category'].str.contains(placeType, na=False)][['safegraph_place_id', 'top_category', 'sub_category', 'naics_code', 'popularity_by_day']]\n",
    "        df = pd.merge(df, df_base[df_base['top_category'].str.contains(placeType, na=False)][['safegraph_place_id', 'popularity_by_day']], on='safegraph_place_id')\n",
    "        df['popularity_by_day_x'] = df['popularity_by_day_x'].apply(lambda x: np.sum(list(literal_eval(x).values())))\n",
    "        df['popularity_by_day_y'] = df['popularity_by_day_y'].apply(lambda x: np.sum(list(literal_eval(x).values())))\n",
    "        df['occupancy'] = df['popularity_by_day_x']/df['popularity_by_day_y']*100\n",
    "        dict_df[placeType] = df\n",
    "    \n",
    "    return dict_df\n",
    "\n",
    "\n",
    "def getPopularity(place_t, df_array, months):\n",
    "    dict_dist = {}\n",
    "    dict_dist[place_t] = {}\n",
    "    \n",
    "    places = [place_types['place_type'][elem] for elem in translation[place_t]]\n",
    "\n",
    "    df_tmp = pd.DataFrame()\n",
    "    for i in range(len(places)):\n",
    "        place = re.escape(places[i])\n",
    "        for df in df_array:\n",
    "            for month in months:\n",
    "                df_tmp = pd.concat([df_tmp, df[month][df[month]['top_category'].str.contains(place, na=False)]])\n",
    "        if df_tmp.empty:\n",
    "            print(\"Data does not contain \"+place)\n",
    "#             return\n",
    "#         try:\n",
    "#             np.concatenate([days, df_tmp['popularity_by_day'].apply(lambda x: literal_eval(x)).values])\n",
    "#         except:\n",
    "    days = df_tmp['popularity_by_day'].apply(lambda x: literal_eval(x)).values\n",
    "    factor_days = dict(functools.reduce(operator.add, map(collections.Counter, days)))\n",
    "    factor_days = {key: (factor_days[key]/factor_days[max(factor_days, key=factor_days.get)]) for key in factor_days.keys()}\n",
    "    for day in factor_days.keys():\n",
    "        dict_dist[place_t][day] = {}\n",
    "\n",
    "        arr = df_tmp['popularity_by_hour'].apply(lambda x: np.array(literal_eval(x))).sum()\n",
    "        arr = (arr - arr[np.argmin(arr)])\n",
    "        arr = arr/arr[np.argmax(arr)]*100*factor_days[day]\n",
    "        dict_dist[place_t][day] = {str(i): arr[i] for i in range(len(arr))}\n",
    "    return dict_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "for year in ['2019', '2020']:\n",
    "        for month in ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']:\n",
    "            keys.append(year+'_'+month)\n",
    "for year in ['2021']:\n",
    "        for month in ['01', '02', '03', '04']:\n",
    "            keys.append(year+'_'+month)\n",
    "            \n",
    "dict_df = {}\n",
    "dict_df_NYC = {}\n",
    "\n",
    "for key in keys:\n",
    "    dict_df[key] = pd.read_csv('../datasets/NYC/NYC_'+key+'.csv.tar.gz', compression='gzip')\n",
    "    \n",
    "dfC_2020_04 = pd.read_csv('../datasets/NYC/Core-NYC_2020_04.csv.tar.gz', compression='gzip')\n",
    "\n",
    "for key in keys:\n",
    "    dict_df_NYC[key] = pd.merge(dfC_2020_04[dfC_2020_04['city']=='New York'][['safegraph_place_id', 'top_category', 'sub_category', 'naics_code']],\n",
    "                                dict_df[key][dict_df[key]['city']=='New York'], on='safegraph_place_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seattle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_Seattle = {}\n",
    "dict_df = {}\n",
    "\n",
    "for key in keys:\n",
    "    dict_df[key] = pd.read_csv('../datasets/SeattleMetro/SeattleMetro_'+key+'.csv.tar.gz', compression='gzip')\n",
    "    \n",
    "dfC_2020_04 = pd.read_csv('../datasets/SeattleMetro/Core-SeattleMetro_2020_04.csv.tar.gz', compression='gzip')\n",
    "\n",
    "for key in keys:\n",
    "    dict_df_Seattle[key] = pd.merge(dfC_2020_04[dfC_2020_04['city']=='Seattle'][['safegraph_place_id', 'top_category', 'sub_category', 'naics_code']],\n",
    "                                dict_df[key][dict_df[key]['city']=='Seattle'], on='safegraph_place_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df_Boston = {}\n",
    "dict_df = {}\n",
    "\n",
    "for key in keys:\n",
    "    dict_df[key] = pd.read_csv('../datasets/Boston/Boston_'+key+'.csv.tar.gz', compression='gzip')\n",
    "    \n",
    "dfC_2020_04 = pd.read_csv('../datasets/Boston/Core-Boston_2020_04.csv.tar.gz', compression='gzip')\n",
    "\n",
    "for key in keys:\n",
    "    dict_df_Boston[key] = pd.merge(dfC_2020_04[dfC_2020_04['city']=='Boston'][['safegraph_place_id', 'top_category', 'sub_category', 'naics_code']],\n",
    "                                dict_df[key][dict_df[key]['city']=='Boston'], on='safegraph_place_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_times = {}\n",
    "for key in translation.keys():\n",
    "    try: \n",
    "        tmp = getPopularity(key, [dict_df_NYC, dict_df_Seattle, dict_df_Boston], keys)\n",
    "    except:\n",
    "        print('Data missing in '+key)\n",
    "        continue\n",
    "    dict_times.update(tmp)\n",
    "dict_times['creation_time_utc'] = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "with open('popular_times.json', 'w') as f:\n",
    "    json.dump(dict_times, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Tutorials)",
   "language": "python",
   "name": "pycharm-38c7cf03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
